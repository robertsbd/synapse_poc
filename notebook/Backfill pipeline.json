{
	"name": "Backfill pipeline",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "smallpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "14c5a5d1-2a0a-4ea8-80f9-8d741766a00f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/2c01d69b-5ddb-449b-8aab-e0a472eeae7f/resourceGroups/synapse-test/providers/Microsoft.Synapse/workspaces/syn-north-europe/bigDataPools/smallpool",
				"name": "smallpool",
				"type": "Spark",
				"endpoint": "https://syn-north-europe.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# An example pipeline based around delta files using Python and SQL\r\n",
					"- This pipeline would be triggered as new data is added to the blob storage, or timed to run on a regular basis, we could even stream data live through it. The entire notebook would be run at once meaning we wouldn't be orcestrating multiple components together.\r\n",
					"- Read CSV files that have been added to blob storage by an external process and write these as delta files to our raw zone\r\n",
					"- Read the delta files in our raw zone, conduct some data cleansing and write to our cleansed zone as well as create tables associated with these files in our cleansed database\r\n",
					"- Read the delta files in our cleaned zone, model them by applying business logic and write these into our model zone as well as creating associated tables in our model database\r\n",
					"\r\n",
					"## Benefits of using delta files\r\n",
					"- Simple: a single file structure in which new data is added, rather than having to combine together multiple files to create the current version of a file\r\n",
					"- Compressed and fast: they are instantiated at the base level as parquet files\r\n",
					"- Can travel back in time to previous versions of the delta file, for instance if problematic data was added and we want to go back to an older version, or we want to see how the data has changed through time\r\n",
					"- ACID transactions which means if we write to the file and there is a failure during the write operation we will not partial data added, transactions are all or nothing"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Helpers\r\n",
					"Define our functions and constants.\r\n",
					"We would aim to build up a code base of reusable functions which would be held din helper "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# This notebook contains setup\r\n",
					"\r\n",
					"# constants\r\n",
					"RAW_DL = 'abfss://rawzone@synapseteststoragebr.dfs.core.windows.net/files/'\r\n",
					"CLEAN_DL = 'abfss://cleanedzone@synapseteststoragebr.dfs.core.windows.net/delta/'\r\n",
					"MODELLED_DL = 'abfss://modelzone@synapseteststoragebr.dfs.core.windows.net/delta/'\r\n",
					"\r\n",
					"SOURCE_SYSTEM = \"adventure_works\" # source system of the data being ingested, \r\n",
					"                                  # should become a parameter\r\n",
					"\r\n",
					"# Example of function we would put in place for cleaning data, this could be made more complete\r\n",
					"\r\n",
					"def clean_dataframe(df_dirty):\r\n",
					"    # names of coumns that we will use to determine unique rows\r\n",
					"    col_names = df_dirty.drop('source', 'pipelineName', 'pipelineTriggerTime').columns\r\n",
					"    return df_dirty.dropDuplicates(col_names) # get rid of any duplicates\r\n",
					"\r\n",
					"# raw_to_clean() will read a delta file from raw and write to a delta file\r\n",
					"# in overwrite mode to cleaned, applying cleaning function\r\n",
					"# here we should verify the schema and also DQ our data, quaranteening any rows that fail\r\n",
					"# note the mode overwrite, note that we are saving as a table to add it to the lake database\r\n",
					"\r\n",
					"def raw_to_clean(rawRelativePath, tableName, lakeDatabase):\r\n",
					"    df_raw = spark.read.load(RAW_DL + rawRelativePath + '/*/*/*/*', format='parquet', header=True)\r\n",
					"    df_cleaned = clean_dataframe(df_raw)\r\n",
					"    df_cleaned.write.option(\"path\", CLEAN_DL + SOURCE_SYSTEM  + \"/\" + tableName). \\\r\n",
					"        format('delta').mode('overwrite').saveAsTable(lakeDatabase + '.' + tableName)\r\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Stage 1: raw zone -> cleaned zone \r\n",
					"- Clean it and save it as delta to the cleansed layer, create catalogue tables in our cleansed lake database\r\n",
					"- Here we provide data that represents the source system tables, cleaned as we require, with data quality checks\r\n",
					"- We also add a catalogue table in our lake database if it doesn't exist, this makes it easy for analysts and others to make use of these tables\r\n",
					"- If the catalogue table already exists then simply by updating delta file itself the data that appears in the table will also be updated, the table is really just a pointer to the file, but it exists in an organised database like structure"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import to_date\r\n",
					"\r\n",
					"# create silver delta files and catelogue table\r\n",
					"raw_to_clean('productcategories', 'productcategories', 'clean')\r\n",
					"raw_to_clean('employees', 'employees', 'clean')\r\n",
					"raw_to_clean('customers', 'customers', 'clean')\r\n",
					"raw_to_clean('products', 'products', 'clean')\r\n",
					"raw_to_clean('productsubcategories', 'productsubcategories', 'clean')\r\n",
					"raw_to_clean('vendorproduct', 'vendorproduct', 'clean')\r\n",
					"raw_to_clean('vendors', 'vendors', 'clean')\r\n",
					"\r\n",
					"# For the table orders we need to change text strings to dates to get rid of US formatting. \r\n",
					"# This is schema enforcement and this is where we would want to start specifying this as metadata\r\n",
					"df_orders = spark.read.load(RAW_DL + 'orders' + '/*/*/*/*', format='parquet', header=True)\r\n",
					"df_orders_out = df_orders.select(\r\n",
					"    df_orders.SalesOrderID,\r\n",
					"    df_orders.SalesOrderDetailID,\r\n",
					"    to_date(df_orders.OrderDate, \"M/d/yyyy\").alias('OrderDate'),\r\n",
					"    to_date(df_orders.DueDate, \"M/d/yyyy\").alias('DueDate'),  \r\n",
					"    to_date(df_orders.ShipDate, \"M/d/yyyy\").alias('ShipDate'),\r\n",
					"    df_orders.EmployeeID,\r\n",
					"    df_orders.CustomerID,\r\n",
					"    df_orders.SubTotal,\r\n",
					"    df_orders.TaxAmt,\r\n",
					"    df_orders.Freight,\r\n",
					"    df_orders.TotalDue,\r\n",
					"    df_orders.ProductID,\r\n",
					"    df_orders.OrderQty,    \r\n",
					"    df_orders.UnitPrice,            \r\n",
					"    df_orders.UnitPriceDiscount,\r\n",
					"    df_orders.LineTotal)\r\n",
					"\r\n",
					"df_cleaned = clean_dataframe(df_orders_out)\r\n",
					"df_cleaned.write.option(\"path\", CLEAN_DL + SOURCE_SYSTEM  + \"/\" + 'orders'). \\\r\n",
					"  format('delta').mode('overwrite').saveAsTable('clean' + '.' + 'orders')\r\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Stage 2: cleaned zone -> model zone\r\n",
					"- Here we transform the data to make it useful for customers and reporting\r\n",
					"- We are creating these as views in a sql database to allow Power BI to query the data"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Business model of ingested data in the cleaned zone\r\n",
					"\r\n",
					"![image-alt-text](https://raw.githubusercontent.com/sfrechette/adventureworks-neo4j/master/graphmodel_adventureworks.png)\r\n",
					"\r\n",
					"We shall create a simpler reporting model by creating a SQL database which can then be queried"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"SELECT * FROM clean.vendors"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}